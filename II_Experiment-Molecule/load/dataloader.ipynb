{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae67e2f",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac3b2a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements: ['*', 'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne', 'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', 'Ga', 'Ge', 'As', 'Se', 'Br', 'Kr', 'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', 'Sb', 'Te', 'I', 'Xe', 'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', 'Yb', 'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi', 'Po', 'At', 'Rn', 'Fr', 'Ra', 'Ac', 'Th', 'Pa', 'U', 'Np', 'Pu', 'Am', 'Cm', 'Bk', 'Cf', 'Es', 'Fm', 'Md', 'No', 'Lr', 'Rf', 'Db', 'Sg', 'Bh', 'Hs', 'Mt', 'Ds', 'Rg', 'Cn', 'Nh', 'Fl', 'Mc', 'Lv', 'Ts', 'Og']\n"
     ]
    }
   ],
   "source": [
    "elements = {\n",
    "    '*': 0, \n",
    "    'H': 1,   'He': 2,\n",
    "    'Li': 3,  'Be': 4,  'B': 5,   'C': 6,   'N': 7,   'O': 8,   'F': 9,   'Ne': 10,\n",
    "    'Na': 11, 'Mg': 12, 'Al': 13, 'Si': 14, 'P': 15,  'S': 16,  'Cl': 17, 'Ar': 18,\n",
    "    'K': 19,  'Ca': 20, 'Sc': 21, 'Ti': 22, 'V': 23,  'Cr': 24, 'Mn': 25, 'Fe': 26, 'Co': 27, 'Ni': 28,'Cu': 29, 'Zn': 30, 'Ga': 31, 'Ge': 32, 'As': 33, 'Se': 34, 'Br': 35, 'Kr': 36,\n",
    "    'Rb': 37, 'Sr': 38, 'Y': 39,  'Zr': 40, 'Nb': 41, 'Mo': 42, 'Tc': 43, 'Ru': 44, 'Rh': 45, 'Pd': 46,'Ag': 47, 'Cd': 48, 'In': 49, 'Sn': 50, 'Sb': 51, 'Te': 52, 'I': 53,  'Xe': 54,\n",
    "    'Cs': 55, 'Ba': 56, 'La': 57, 'Ce': 58, 'Pr': 59, 'Nd': 60, 'Pm': 61, 'Sm': 62, 'Eu': 63, 'Gd': 64,'Tb': 65, 'Dy': 66, 'Ho': 67, 'Er': 68, 'Tm': 69, 'Yb': 70, 'Lu': 71,'Hf': 72, 'Ta': 73, 'W': 74,  'Re': 75, 'Os': 76, 'Ir': 77, 'Pt': 78, 'Au': 79, 'Hg': 80,'Tl': 81, 'Pb': 82, 'Bi': 83, 'Po': 84, 'At': 85, 'Rn': 86,\n",
    "    'Fr': 87, 'Ra': 88, 'Ac': 89, 'Th': 90, 'Pa': 91, 'U': 92,  'Np': 93, 'Pu': 94, 'Am': 95, 'Cm': 96,'Bk': 97, 'Cf': 98, 'Es': 99, 'Fm': 100, 'Md': 101, 'No': 102, 'Lr': 103,'Rf': 104, 'Db': 105, 'Sg': 106, 'Bh': 107, 'Hs': 108, 'Mt': 109, 'Ds': 110, 'Rg': 111,'Cn': 112, 'Nh': 113, 'Fl': 114, 'Mc': 115, 'Lv': 116, 'Ts': 117, 'Og': 118\n",
    "}\n",
    "\n",
    "element_list = list(elements.keys())\n",
    "element_to_idx = {el: idx for idx, el in enumerate(element_list)}\n",
    "print(f\"Elements: {element_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40236864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(element):\n",
    "    vec = torch.zeros(len(element_list), dtype=torch.float32)\n",
    "    idx = element_to_idx.get(element, element_to_idx['*']) \n",
    "    vec[idx] = 1.0\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e58e9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "\n",
    "torch_version = torch.__version__.split(\"+\")\n",
    "os.environ[\"TORCH\"] = torch_version[0]\n",
    "os.environ[\"CUDA\"] = torch_version[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9a80abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_polymers(polymers_dir):\n",
    "    polymer_graphs = []\n",
    "\n",
    "    folders = [folder for folder in os.listdir(polymers_dir) if os.path.isdir(os.path.join(polymers_dir, folder))]\n",
    "    folders = sorted(folders)  \n",
    "\n",
    "    for folder in tqdm(folders):\n",
    "        folder_path = os.path.join(polymers_dir, folder)\n",
    "\n",
    "        adj_path = os.path.join(folder_path, 'adjacency.csv')\n",
    "        atom_path = os.path.join(folder_path, 'atoms.csv')\n",
    "        details_path = os.path.join(folder_path, 'details.csv')\n",
    "\n",
    "        if not os.path.exists(adj_path) or not os.path.exists(atom_path):\n",
    "            continue\n",
    "\n",
    "        adj = pd.read_csv(adj_path).values\n",
    "        n_nodes = adj.shape[0]\n",
    "\n",
    "        atom_df = pd.read_csv(atom_path)\n",
    "        atom_labels = dict(zip(atom_df['node'], atom_df['element']))\n",
    "\n",
    "        x = torch.stack([\n",
    "            one_hot(atom_labels.get(i, '*'))\n",
    "            for i in range(n_nodes)\n",
    "        ])\n",
    "\n",
    "        src, dst = np.where(adj != 0)\n",
    "        edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        data.id = folder  \n",
    "\n",
    "        details_df = pd.read_csv(details_path)\n",
    "        details = details_df.iloc[0].to_dict()\n",
    "        for key, value in details.items():\n",
    "            try:\n",
    "                val = float(value)\n",
    "            except:\n",
    "                val = float('nan')\n",
    "            data[key] = torch.tensor([val], dtype=torch.float32)\n",
    "\n",
    "        polymer_graphs.append(data)\n",
    "\n",
    "    return polymer_graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0804ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def split_polymer_data_by_attributes(polymers, output_dir, train_ratio=0.6, val_ratio=0.2, seed=42):\n",
    "\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    BASE_KEYS = {'x', 'edge_index', 'id'}\n",
    "    \n",
    "    target_attrs = set()\n",
    "    for p in polymers:\n",
    "        for key in p.keys():\n",
    "            if key in BASE_KEYS:\n",
    "                continue\n",
    "            val = p[key]\n",
    "            if isinstance(val, torch.Tensor) and val.numel() == 1:\n",
    "                target_attrs.add(key)\n",
    "    \n",
    "    target_attrs = sorted(list(target_attrs))\n",
    "    print(f\"Target attributes: {target_attrs}\")\n",
    "    \n",
    "    for attr in target_attrs:\n",
    "        filtered = []\n",
    "        for p in polymers:\n",
    "            if attr not in p:\n",
    "                continue\n",
    "\n",
    "            v = p[attr]\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                val = v.item()\n",
    "            else:\n",
    "                val = float(v)\n",
    "\n",
    "            if np.isnan(val):\n",
    "                continue\n",
    "\n",
    "            data = Data(\n",
    "                x=p['x'],\n",
    "                edge_index=p['edge_index'],\n",
    "                y=torch.tensor([val], dtype=torch.float32),\n",
    "                id=p['id']\n",
    "            )\n",
    "            filtered.append(data)\n",
    "\n",
    "        if len(filtered) == 0:\n",
    "            print(f\"{attr}: No valid data, skipping\")\n",
    "            continue\n",
    "            \n",
    "        random.shuffle(filtered)\n",
    "\n",
    "        n = len(filtered)\n",
    "        n_train = int(n * train_ratio)\n",
    "        n_val = int(n * val_ratio)\n",
    "        n_test = n - n_train - n_val\n",
    "\n",
    "        train_set = filtered[:n_train]\n",
    "        validation_set = filtered[n_train:n_train + n_val]\n",
    "        test_set = filtered[n_train + n_val:]\n",
    "\n",
    "        base_path = os.path.join(output_dir, attr)\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "        torch.save(train_set, os.path.join(base_path, 'train.pt'))\n",
    "        torch.save(validation_set, os.path.join(base_path, 'val.pt'))\n",
    "        torch.save(test_set, os.path.join(base_path, 'test.pt'))\n",
    "\n",
    "        print(f\"{attr} saved\")\n",
    "    \n",
    "    print(f\"\\nDone\")\n",
    "    return target_attrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea0567",
   "metadata": {},
   "source": [
    "## ESOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feecf8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1127 [00:00<?, ?it/s]/tmp/ipykernel_282910/4060424486.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
      "100%|██████████| 1127/1127 [00:07<00:00, 155.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target attributes: ['ESOL predicted log solubility in mols per litre']\n",
      "ESOL predicted log solubility in mols per litre saved\n",
      "\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ESOL predicted log solubility in mols per litre']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polymers = load_all_polymers('../../kaggle/ESOL/')\n",
    "split_polymer_data_by_attributes(polymers, output_dir='../data/ESOL/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cddec7",
   "metadata": {},
   "source": [
    "## Lipophilicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "099d44f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4157/4157 [00:29<00:00, 139.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target attributes: ['exp']\n",
      "exp saved\n",
      "\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['exp']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polymers = load_all_polymers('../../kaggle/Lipophilicity/')\n",
    "split_polymer_data_by_attributes(polymers, output_dir='../data/Lipophilicity/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2078639",
   "metadata": {},
   "source": [
    "## NeurIps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc0357f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7949 [00:00<?, ?it/s]/tmp/ipykernel_2984694/4060424486.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
      "100%|██████████| 7949/7949 [00:49<00:00, 162.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target attributes: ['Density', 'FFV', 'Rg', 'Tc', 'Tg']\n",
      "Density saved\n",
      "FFV saved\n",
      "Rg saved\n",
      "Tc saved\n",
      "Tg saved\n",
      "\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Density', 'FFV', 'Rg', 'Tc', 'Tg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polymers = load_all_polymers('../../kaggle/NeurIPS/')\n",
    "split_polymer_data_by_attributes(polymers, output_dir='../data/NeurIPS/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
